{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import chromadb\n",
    "import ollama\n",
    "from ollama import embed\n",
    "from IPython.display import Markdown\n",
    "from ollama import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
    "# DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
    "# DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
    "\n",
    "# documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT1 = \"\"\"\n",
    "Sure! Let‚Äôs break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they‚Äôre used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** ‚Äî especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What Are BLEU and ROUGE?\n",
    "\n",
    "### **BLEU (Bilingual Evaluation Understudy)**\n",
    "- Originally designed to **evaluate machine translation**.\n",
    "- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\n",
    "- Focuses on **precision** ‚Äî how much of the generated text overlaps with the reference.\n",
    "\n",
    "**Example:**\n",
    "If the model generates:  \n",
    "`\"The cat is on the mat\"`  \n",
    "and the reference is:  \n",
    "`\"The cat sat on the mat\"`  \n",
    "BLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\n",
    "\n",
    "### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "- Developed for **evaluating automatic summarization**.\n",
    "- Measures **how much of the reference text appears in the generated text**, again using n-grams.\n",
    "- Focuses more on **recall** ‚Äî how much of the reference content is captured.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How Are These Metrics Used in Evaluating LLMs?\n",
    "\n",
    "BLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They're commonly applied in these contexts:\n",
    "\n",
    "### 1. **Prompt Engineering**\n",
    "- When experimenting with different prompts to elicit better responses from an LLM:\n",
    "  - BLEU/ROUGE can **score how close the output is to a desired answer**.\n",
    "  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\n",
    "\n",
    "### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\n",
    "- In complex domains like medicine or security:\n",
    "  - There may not be one \"correct\" answer.\n",
    "  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\n",
    "  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\n",
    "\n",
    "### 3. **Text Generation (Machine Translation, Summarization)**\n",
    "- In foundational NLP tasks:\n",
    "  - **BLEU is used for translations** ‚Äî does the output match human-translated sentences?\n",
    "  - **ROUGE is used for summaries** ‚Äî does the summary include key phrases from the original content?\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Limitations of BLEU and ROUGE\n",
    "\n",
    "While useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\n",
    "\n",
    "- They **penalize valid but novel responses** ‚Äî because the generated text might be correct or high quality, even if it doesn't match the reference exactly.\n",
    "- They **struggle with semantic equivalence** ‚Äî they don‚Äôt \"understand meaning\", just surface word overlaps.\n",
    "- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ The Bigger Picture: Evaluating LLMs\n",
    "\n",
    "To properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\n",
    "\n",
    "### ‚úÖ **Traditional Metrics (like BLEU/ROUGE)**\n",
    "- Quick, objective, and reproducible.\n",
    "- Best for **structured tasks** with clear reference outputs.\n",
    "\n",
    "### ‚úÖ **Human Evaluation**\n",
    "- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\n",
    "- But expensive and not easily scalable.\n",
    "\n",
    "### ‚úÖ **LLM-Powered Auto-Raters**\n",
    "- Use another calibrated language model to **simulate human evaluation**.\n",
    "- Can scale more easily than human raters.\n",
    "- Requires **calibration against human judgments** to ensure reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Final Thoughts\n",
    "\n",
    "So, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\n",
    "\n",
    "- **Similarity-based automatic metrics** (like BLEU/ROUGE),\n",
    "- **Human assessments**, and\n",
    "- **LLM-based scoring systems**.\n",
    "\n",
    "This ensures that both **objective correctness** and **subjective quality** are properly captured ‚Äî especially important when working with LLMs in nuanced or high-stakes domains.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT2 = \"\"\"\n",
    "Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\n",
    "\n",
    "Here's a breakdown of the main types of word embeddings:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Count-Based Embeddings (Matrix Factorization)**\n",
    "These are derived from word co-occurrence matrices.\n",
    "\n",
    "#### a. **Latent Semantic Analysis (LSA)**\n",
    "- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\n",
    "- **Captures**: Word similarity based on co-occurrence.\n",
    "- **Limitations**: Linear, doesn't capture context dynamically.\n",
    "\n",
    "#### b. **Pointwise Mutual Information (PMI) + SVD**\n",
    "- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\n",
    "- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Predictive Embeddings (Neural Networks)**\n",
    "These use shallow neural networks to predict word context.\n",
    "\n",
    "#### a. **Word2Vec**\n",
    "- **Models**:\n",
    "  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\n",
    "  - **Skip-Gram**: Predicts context words from a given word.\n",
    "- **Training**: Negative sampling or hierarchical softmax.\n",
    "- **Captures**: Semantic similarity, analogies (`king - man + woman ‚âà queen`).\n",
    "\n",
    "```python\n",
    "# Using gensim to load Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "```\n",
    "\n",
    "#### b. **GloVe (Global Vectors for Word Representation)**\n",
    "- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\n",
    "- **Captures**: Global co-occurrence and local context.\n",
    "- **Advantage**: Incorporates both count-based and predictive elements.\n",
    "\n",
    "#### c. **FastText**\n",
    "- **Extension of Word2Vec** developed by Facebook.\n",
    "- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\n",
    "- Better for morphologically rich languages.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Contextual Embeddings**\n",
    "These generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\n",
    "\n",
    "#### a. **ELMo (Embeddings from Language Models)**\n",
    "- Uses a **bi-directional LSTM** over the entire sentence.\n",
    "- **Context-aware**: Embeddings vary by usage.\n",
    "\n",
    "#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "- Uses Transformer encoders.\n",
    "- Learns embeddings for entire sentences, with **deep bidirectional** context.\n",
    "- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\n",
    "\n",
    "```python\n",
    "# Example using Hugging Face Transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "word_embeddings = outputs.last_hidden_state\n",
    "```\n",
    "\n",
    "#### c. **Other Transformer Models**\n",
    "- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\n",
    "- All provide contextual embeddings, fine-tuned for various tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Type              | Examples         | Contextual | Handles OOV | Architecture        |\n",
    "|-------------------|------------------|------------|-------------|---------------------|\n",
    "| Count-based       | LSA, PMI         | ‚ùå         | ‚ùå          | Matrix factorization |\n",
    "| Predictive        | Word2Vec, GloVe  | ‚ùå         | FastText: ‚úÖ| Shallow NN          |\n",
    "| Contextual        | ELMo, BERT, GPT  | ‚úÖ         | ‚úÖ          | RNNs / Transformers |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT3 = \"\"\"\n",
    "Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\n",
    "\n",
    "Here's a breakdown of its significance in the context of LLMs:\n",
    "\n",
    "*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs. Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\n",
    "\n",
    "*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**. It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs. For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\n",
    "\n",
    "*   **How Self-Attention Works:** The process involves several steps:\n",
    "    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should 'attend' to other words. This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\n",
    "    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights. These weights indicate the strength of connection between words.\n",
    "    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\n",
    "    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\n",
    "\n",
    "*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel. Each 'head' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\n",
    "\n",
    "*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\n",
    "\n",
    "*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply. Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\n",
    "\n",
    "*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process. Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\n",
    "\n",
    "In summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks. Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        response = embed(model='bge-m3', input=input)\n",
    "        return response['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"testdb1\"\n",
    "chroma_client = chromadb.Client()\n",
    "embed_fn = OllamaEmbeddingFunction()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['0', '1'],\n",
       " 'embeddings': array([[-0.00175362, -0.00174541,  0.00709319, ..., -0.0428962 ,\n",
       "          0.01390002,  0.00645761],\n",
       "        [-0.02732202,  0.00666147,  0.00107146, ..., -0.01805371,\n",
       "         -0.01269804,  0.00306338]], shape=(2, 1024)),\n",
       " 'documents': ['\\nSure! Let‚Äôs break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they‚Äôre used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** ‚Äî especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## üîπ What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** ‚Äî how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** ‚Äî how much of the reference content is captured.\\n\\n---\\n\\n## üîπ How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They\\'re commonly applied in these contexts:\\n\\n### 1. **Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3. **Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** ‚Äî does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** ‚Äî does the summary include key phrases from the original content?\\n\\n---\\n\\n## üîπ Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** ‚Äî because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** ‚Äî they don‚Äôt \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## üîπ The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ‚úÖ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ‚úÖ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ‚úÖ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## üîπ Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured ‚Äî especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?\\n',\n",
       "  '\\nWord embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere\\'s a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1. Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a. **Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn\\'t capture context dynamically.\\n\\n#### b. **Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2. Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a. **Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ‚âà queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b. **GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c. **FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3. Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a. **ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c. **Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ‚ùå         | ‚ùå          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ‚ùå         | FastText: ‚úÖ| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ‚úÖ         | ‚úÖ          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step).'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [None, None],\n",
       " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.peek(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Sure! Let‚Äôs break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they‚Äôre used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** ‚Äî especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\n",
       "\n",
       "---\n",
       "\n",
       "## üîπ What Are BLEU and ROUGE?\n",
       "\n",
       "### **BLEU (Bilingual Evaluation Understudy)**\n",
       "- Originally designed to **evaluate machine translation**.\n",
       "- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\n",
       "- Focuses on **precision** ‚Äî how much of the generated text overlaps with the reference.\n",
       "\n",
       "**Example:**\n",
       "If the model generates:  \n",
       "`\"The cat is on the mat\"`  \n",
       "and the reference is:  \n",
       "`\"The cat sat on the mat\"`  \n",
       "BLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\n",
       "\n",
       "### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
       "- Developed for **evaluating automatic summarization**.\n",
       "- Measures **how much of the reference text appears in the generated text**, again using n-grams.\n",
       "- Focuses more on **recall** ‚Äî how much of the reference content is captured.\n",
       "\n",
       "---\n",
       "\n",
       "## üîπ How Are These Metrics Used in Evaluating LLMs?\n",
       "\n",
       "BLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They're commonly applied in these contexts:\n",
       "\n",
       "### 1. **Prompt Engineering**\n",
       "- When experimenting with different prompts to elicit better responses from an LLM:\n",
       "  - BLEU/ROUGE can **score how close the output is to a desired answer**.\n",
       "  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\n",
       "\n",
       "### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\n",
       "- In complex domains like medicine or security:\n",
       "  - There may not be one \"correct\" answer.\n",
       "  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\n",
       "  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\n",
       "\n",
       "### 3. **Text Generation (Machine Translation, Summarization)**\n",
       "- In foundational NLP tasks:\n",
       "  - **BLEU is used for translations** ‚Äî does the output match human-translated sentences?\n",
       "  - **ROUGE is used for summaries** ‚Äî does the summary include key phrases from the original content?\n",
       "\n",
       "---\n",
       "\n",
       "## üîπ Limitations of BLEU and ROUGE\n",
       "\n",
       "While useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\n",
       "\n",
       "- They **penalize valid but novel responses** ‚Äî because the generated text might be correct or high quality, even if it doesn't match the reference exactly.\n",
       "- They **struggle with semantic equivalence** ‚Äî they don‚Äôt \"understand meaning\", just surface word overlaps.\n",
       "- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\n",
       "\n",
       "---\n",
       "\n",
       "## üîπ The Bigger Picture: Evaluating LLMs\n",
       "\n",
       "To properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\n",
       "\n",
       "### ‚úÖ **Traditional Metrics (like BLEU/ROUGE)**\n",
       "- Quick, objective, and reproducible.\n",
       "- Best for **structured tasks** with clear reference outputs.\n",
       "\n",
       "### ‚úÖ **Human Evaluation**\n",
       "- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\n",
       "- But expensive and not easily scalable.\n",
       "\n",
       "### ‚úÖ **LLM-Powered Auto-Raters**\n",
       "- Use another calibrated language model to **simulate human evaluation**.\n",
       "- Can scale more easily than human raters.\n",
       "- Requires **calibration against human judgments** to ensure reliability.\n",
       "\n",
       "---\n",
       "\n",
       "## üîπ Final Thoughts\n",
       "\n",
       "So, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\n",
       "\n",
       "- **Similarity-based automatic metrics** (like BLEU/ROUGE),\n",
       "- **Human assessments**, and\n",
       "- **LLM-based scoring systems**.\n",
       "\n",
       "This ensures that both **objective correctness** and **subjective quality** are properly captured ‚Äî especially important when working with LLMs in nuanced or high-stakes domains.\n",
       "\n",
       "---\n",
       "\n",
       "Would you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the limitations of BLEU and ROUGE\"\n",
    "result = db.query(query_texts=[query], n_results=1)\n",
    "[all_passages] = result[\"documents\"]\n",
    "Markdown(all_passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
      "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
      "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
      "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
      "\n",
      "QUESTION: What are the limitations of BLEU and ROUGE\n",
      "PASSAGE:  Sure! Let‚Äôs break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they‚Äôre used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** ‚Äî especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.  ---  ## üîπ What Are BLEU and ROUGE?  ### **BLEU (Bilingual Evaluation Understudy)** - Originally designed to **evaluate machine translation**. - Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams). - Focuses on **precision** ‚Äî how much of the generated text overlaps with the reference.  **Example:** If the model generates:   `\"The cat is on the mat\"`   and the reference is:   `\"The cat sat on the mat\"`   BLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.  ### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** - Developed for **evaluating automatic summarization**. - Measures **how much of the reference text appears in the generated text**, again using n-grams. - Focuses more on **recall** ‚Äî how much of the reference content is captured.  ---  ## üîπ How Are These Metrics Used in Evaluating LLMs?  BLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They're commonly applied in these contexts:  ### 1. **Prompt Engineering** - When experimenting with different prompts to elicit better responses from an LLM:   - BLEU/ROUGE can **score how close the output is to a desired answer**.   - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).  ### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)** - In complex domains like medicine or security:   - There may not be one \"correct\" answer.   - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).   - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.  ### 3. **Text Generation (Machine Translation, Summarization)** - In foundational NLP tasks:   - **BLEU is used for translations** ‚Äî does the output match human-translated sentences?   - **ROUGE is used for summaries** ‚Äî does the summary include key phrases from the original content?  ---  ## üîπ Limitations of BLEU and ROUGE  While useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:  - They **penalize valid but novel responses** ‚Äî because the generated text might be correct or high quality, even if it doesn't match the reference exactly. - They **struggle with semantic equivalence** ‚Äî they don‚Äôt \"understand meaning\", just surface word overlaps. - This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.  ---  ## üîπ The Bigger Picture: Evaluating LLMs  To properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:  ### ‚úÖ **Traditional Metrics (like BLEU/ROUGE)** - Quick, objective, and reproducible. - Best for **structured tasks** with clear reference outputs.  ### ‚úÖ **Human Evaluation** - Gold standard for assessing **fluency, relevance, coherence, and creativity**. - But expensive and not easily scalable.  ### ‚úÖ **LLM-Powered Auto-Raters** - Use another calibrated language model to **simulate human evaluation**. - Can scale more easily than human raters. - Requires **calibration against human judgments** to ensure reliability.  ---  ## üîπ Final Thoughts  So, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:  - **Similarity-based automatic metrics** (like BLEU/ROUGE), - **Human assessments**, and - **LLM-based scoring systems**.  This ensures that both **objective correctness** and **subjective quality** are properly captured ‚Äî especially important when working with LLMs in nuanced or high-stakes domains.  ---  Would you like a visual summary or code example showing how BLEU or ROUGE is computed in practice? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_oneline = query.replace(\"\\n\", \" \")\n",
    "## Concise answer prompt\n",
    "# prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
    "# Be sure to respond in a complete sentence, being concise, including only relevant information. \n",
    "# However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
    "# strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "# QUESTION: {query_oneline}\n",
    "# \"\"\"\n",
    "\n",
    "## Detailed answer prompt\n",
    "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
    "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
    "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
    "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "QUESTION: {query_oneline}\n",
    "\"\"\"\n",
    "\n",
    "# Add the retrieved documents to the prompt.\n",
    "for passage in all_passages:\n",
    "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
    "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate('llama3.2', prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "BLEU (Bilingual Evaluation Understudy) and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) are two popular metrics used to evaluate the quality of text generated by Large Language Models (LLMs). In simple terms, they measure how similar a generated sentence is to one or more reference sentences, based on matching sequences of words (n-grams). However, these metrics have limitations, especially when evaluating creative or open-ended tasks. They can penalize valid but novel responses and struggle with semantic equivalence, meaning they don't truly understand the meaning behind the words.\n",
       "\n",
       "BLEU focuses on precision, looking at how much of the generated text overlaps with the reference, while ROUGE focuses more on recall, measuring how much of the reference text appears in the generated text. For example, if a model generates \"The cat is on the mat\" and the reference is \"The cat sat on the mat,\" BLEU would look at the overlapping n-grams like \"the,\" \"cat,\" \"on,\" \"the,\" and \"mat,\" and calculate a score.\n",
       "\n",
       "BLEU and ROUGE are commonly used in tasks like prompt engineering, machine translation, summarization, and domain-specific applications. They provide an objective, quantitative way to evaluate the quality of text generated by LLMs. However, relying solely on these metrics is not enough for a comprehensive evaluation. A multi-method approach that combines similarity-based automatic metrics with human assessments and LLM-powered auto-raters is recommended.\n",
       "\n",
       "For instance, in machine translation, BLEU is used to score how close the output is to a desired answer, while ROUGE is used in summarization to check if the summary includes key phrases from the original content. However, when working with LLMs in nuanced or high-stakes domains, such as medicine or security, these metrics may not be enough. In such cases, combining objective correctness with subjective quality assessments is crucial.\n",
       "\n",
       "In conclusion, while BLEU and ROUGE are valuable tools for evaluating LLMs, especially in structured tasks with clear reference outputs, they should be used in conjunction with human evaluations and other scoring systems to ensure a comprehensive assessment of the model's performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
