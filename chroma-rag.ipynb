{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import chromadb\n",
    "import ollama\n",
    "from ollama import embed\n",
    "from IPython.display import Markdown\n",
    "from ollama import generate\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
    "# DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
    "# DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
    "\n",
    "# documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT1 = \"\"\"\n",
    "Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 What Are BLEU and ROUGE?\n",
    "\n",
    "### **BLEU (Bilingual Evaluation Understudy)**\n",
    "- Originally designed to **evaluate machine translation**.\n",
    "- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\n",
    "- Focuses on **precision** — how much of the generated text overlaps with the reference.\n",
    "\n",
    "**Example:**\n",
    "If the model generates:  \n",
    "`\"The cat is on the mat\"`  \n",
    "and the reference is:  \n",
    "`\"The cat sat on the mat\"`  \n",
    "BLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\n",
    "\n",
    "### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "- Developed for **evaluating automatic summarization**.\n",
    "- Measures **how much of the reference text appears in the generated text**, again using n-grams.\n",
    "- Focuses more on **recall** — how much of the reference content is captured.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 How Are These Metrics Used in Evaluating LLMs?\n",
    "\n",
    "BLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They're commonly applied in these contexts:\n",
    "\n",
    "### 1. **Prompt Engineering**\n",
    "- When experimenting with different prompts to elicit better responses from an LLM:\n",
    "  - BLEU/ROUGE can **score how close the output is to a desired answer**.\n",
    "  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\n",
    "\n",
    "### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\n",
    "- In complex domains like medicine or security:\n",
    "  - There may not be one \"correct\" answer.\n",
    "  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\n",
    "  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\n",
    "\n",
    "### 3. **Text Generation (Machine Translation, Summarization)**\n",
    "- In foundational NLP tasks:\n",
    "  - **BLEU is used for translations** — does the output match human-translated sentences?\n",
    "  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Limitations of BLEU and ROUGE\n",
    "\n",
    "While useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\n",
    "\n",
    "- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn't match the reference exactly.\n",
    "- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\n",
    "- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 The Bigger Picture: Evaluating LLMs\n",
    "\n",
    "To properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\n",
    "\n",
    "### ✅ **Traditional Metrics (like BLEU/ROUGE)**\n",
    "- Quick, objective, and reproducible.\n",
    "- Best for **structured tasks** with clear reference outputs.\n",
    "\n",
    "### ✅ **Human Evaluation**\n",
    "- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\n",
    "- But expensive and not easily scalable.\n",
    "\n",
    "### ✅ **LLM-Powered Auto-Raters**\n",
    "- Use another calibrated language model to **simulate human evaluation**.\n",
    "- Can scale more easily than human raters.\n",
    "- Requires **calibration against human judgments** to ensure reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Final Thoughts\n",
    "\n",
    "So, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\n",
    "\n",
    "- **Similarity-based automatic metrics** (like BLEU/ROUGE),\n",
    "- **Human assessments**, and\n",
    "- **LLM-based scoring systems**.\n",
    "\n",
    "This ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT2 = \"\"\"\n",
    "Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\n",
    "\n",
    "Here's a breakdown of the main types of word embeddings:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Count-Based Embeddings (Matrix Factorization)**\n",
    "These are derived from word co-occurrence matrices.\n",
    "\n",
    "#### a. **Latent Semantic Analysis (LSA)**\n",
    "- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\n",
    "- **Captures**: Word similarity based on co-occurrence.\n",
    "- **Limitations**: Linear, doesn't capture context dynamically.\n",
    "\n",
    "#### b. **Pointwise Mutual Information (PMI) + SVD**\n",
    "- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\n",
    "- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Predictive Embeddings (Neural Networks)**\n",
    "These use shallow neural networks to predict word context.\n",
    "\n",
    "#### a. **Word2Vec**\n",
    "- **Models**:\n",
    "  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\n",
    "  - **Skip-Gram**: Predicts context words from a given word.\n",
    "- **Training**: Negative sampling or hierarchical softmax.\n",
    "- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\n",
    "\n",
    "```python\n",
    "# Using gensim to load Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "```\n",
    "\n",
    "#### b. **GloVe (Global Vectors for Word Representation)**\n",
    "- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\n",
    "- **Captures**: Global co-occurrence and local context.\n",
    "- **Advantage**: Incorporates both count-based and predictive elements.\n",
    "\n",
    "#### c. **FastText**\n",
    "- **Extension of Word2Vec** developed by Facebook.\n",
    "- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\n",
    "- Better for morphologically rich languages.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Contextual Embeddings**\n",
    "These generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\n",
    "\n",
    "#### a. **ELMo (Embeddings from Language Models)**\n",
    "- Uses a **bi-directional LSTM** over the entire sentence.\n",
    "- **Context-aware**: Embeddings vary by usage.\n",
    "\n",
    "#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "- Uses Transformer encoders.\n",
    "- Learns embeddings for entire sentences, with **deep bidirectional** context.\n",
    "- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\n",
    "\n",
    "```python\n",
    "# Example using Hugging Face Transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "word_embeddings = outputs.last_hidden_state\n",
    "```\n",
    "\n",
    "#### c. **Other Transformer Models**\n",
    "- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\n",
    "- All provide contextual embeddings, fine-tuned for various tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Type              | Examples         | Contextual | Handles OOV | Architecture        |\n",
    "|-------------------|------------------|------------|-------------|---------------------|\n",
    "| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\n",
    "| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\n",
    "| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT3 = \"\"\"\n",
    "Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\n",
    "\n",
    "Here's a breakdown of its significance in the context of LLMs:\n",
    "\n",
    "*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs. Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\n",
    "\n",
    "*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**. It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs. For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\n",
    "\n",
    "*   **How Self-Attention Works:** The process involves several steps:\n",
    "    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should 'attend' to other words. This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\n",
    "    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights. These weights indicate the strength of connection between words.\n",
    "    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\n",
    "    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\n",
    "\n",
    "*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel. Each 'head' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\n",
    "\n",
    "*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\n",
    "\n",
    "*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply. Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\n",
    "\n",
    "*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process. Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\n",
    "\n",
    "In summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks. Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document Embeddings\n",
    "\n",
    "## Create document embeddings and add to chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        response = embed(model='bge-m3', input=input)\n",
    "        return response['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"testdb1\"\n",
    "chroma_client = chromadb.Client()\n",
    "embed_fn = OllamaEmbeddingFunction()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['0', '1'],\n",
       " 'embeddings': array([[-0.00175362, -0.00174541,  0.00709319, ..., -0.0428962 ,\n",
       "          0.01390002,  0.00645761],\n",
       "        [-0.02732202,  0.00666147,  0.00107146, ..., -0.01805371,\n",
       "         -0.01269804,  0.00306338]], shape=(2, 1024)),\n",
       " 'documents': ['\\nSure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They\\'re commonly applied in these contexts:\\n\\n### 1. **Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3. **Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?\\n',\n",
       "  '\\nWord embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere\\'s a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1. Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a. **Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn\\'t capture context dynamically.\\n\\n#### b. **Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2. Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a. **Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b. **GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c. **FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3. Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a. **ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c. **Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step).'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [None, None],\n",
       " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.peek(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query chromadb for embeddings and get relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\n",
       "\n",
       "---\n",
       "\n",
       "## 🔹 What Are BLEU and ROUGE?\n",
       "\n",
       "### **BLEU (Bilingual Evaluation Understudy)**\n",
       "- Originally designed to **evaluate machine translation**.\n",
       "- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\n",
       "- Focuses on **precision** — how much of the generated text overlaps with the reference.\n",
       "\n",
       "**Example:**\n",
       "If the model generates:  \n",
       "`\"The cat is on the mat\"`  \n",
       "and the reference is:  \n",
       "`\"The cat sat on the mat\"`  \n",
       "BLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\n",
       "\n",
       "### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
       "- Developed for **evaluating automatic summarization**.\n",
       "- Measures **how much of the reference text appears in the generated text**, again using n-grams.\n",
       "- Focuses more on **recall** — how much of the reference content is captured.\n",
       "\n",
       "---\n",
       "\n",
       "## 🔹 How Are These Metrics Used in Evaluating LLMs?\n",
       "\n",
       "BLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They're commonly applied in these contexts:\n",
       "\n",
       "### 1. **Prompt Engineering**\n",
       "- When experimenting with different prompts to elicit better responses from an LLM:\n",
       "  - BLEU/ROUGE can **score how close the output is to a desired answer**.\n",
       "  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\n",
       "\n",
       "### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\n",
       "- In complex domains like medicine or security:\n",
       "  - There may not be one \"correct\" answer.\n",
       "  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\n",
       "  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\n",
       "\n",
       "### 3. **Text Generation (Machine Translation, Summarization)**\n",
       "- In foundational NLP tasks:\n",
       "  - **BLEU is used for translations** — does the output match human-translated sentences?\n",
       "  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\n",
       "\n",
       "---\n",
       "\n",
       "## 🔹 Limitations of BLEU and ROUGE\n",
       "\n",
       "While useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\n",
       "\n",
       "- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn't match the reference exactly.\n",
       "- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\n",
       "- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\n",
       "\n",
       "---\n",
       "\n",
       "## 🔹 The Bigger Picture: Evaluating LLMs\n",
       "\n",
       "To properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\n",
       "\n",
       "### ✅ **Traditional Metrics (like BLEU/ROUGE)**\n",
       "- Quick, objective, and reproducible.\n",
       "- Best for **structured tasks** with clear reference outputs.\n",
       "\n",
       "### ✅ **Human Evaluation**\n",
       "- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\n",
       "- But expensive and not easily scalable.\n",
       "\n",
       "### ✅ **LLM-Powered Auto-Raters**\n",
       "- Use another calibrated language model to **simulate human evaluation**.\n",
       "- Can scale more easily than human raters.\n",
       "- Requires **calibration against human judgments** to ensure reliability.\n",
       "\n",
       "---\n",
       "\n",
       "## 🔹 Final Thoughts\n",
       "\n",
       "So, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\n",
       "\n",
       "- **Similarity-based automatic metrics** (like BLEU/ROUGE),\n",
       "- **Human assessments**, and\n",
       "- **LLM-based scoring systems**.\n",
       "\n",
       "This ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\n",
       "\n",
       "---\n",
       "\n",
       "Would you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the limitations of BLEU and ROUGE\"\n",
    "result = db.query(query_texts=[query], n_results=1)\n",
    "[all_passages] = result[\"documents\"]\n",
    "Markdown(all_passages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
      "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
      "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
      "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
      "\n",
      "QUESTION: What are the limitations of BLEU and ROUGE\n",
      "PASSAGE:  Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.  ---  ## 🔹 What Are BLEU and ROUGE?  ### **BLEU (Bilingual Evaluation Understudy)** - Originally designed to **evaluate machine translation**. - Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams). - Focuses on **precision** — how much of the generated text overlaps with the reference.  **Example:** If the model generates:   `\"The cat is on the mat\"`   and the reference is:   `\"The cat sat on the mat\"`   BLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.  ### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** - Developed for **evaluating automatic summarization**. - Measures **how much of the reference text appears in the generated text**, again using n-grams. - Focuses more on **recall** — how much of the reference content is captured.  ---  ## 🔹 How Are These Metrics Used in Evaluating LLMs?  BLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They're commonly applied in these contexts:  ### 1. **Prompt Engineering** - When experimenting with different prompts to elicit better responses from an LLM:   - BLEU/ROUGE can **score how close the output is to a desired answer**.   - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).  ### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)** - In complex domains like medicine or security:   - There may not be one \"correct\" answer.   - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).   - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.  ### 3. **Text Generation (Machine Translation, Summarization)** - In foundational NLP tasks:   - **BLEU is used for translations** — does the output match human-translated sentences?   - **ROUGE is used for summaries** — does the summary include key phrases from the original content?  ---  ## 🔹 Limitations of BLEU and ROUGE  While useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:  - They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn't match the reference exactly. - They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps. - This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.  ---  ## 🔹 The Bigger Picture: Evaluating LLMs  To properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:  ### ✅ **Traditional Metrics (like BLEU/ROUGE)** - Quick, objective, and reproducible. - Best for **structured tasks** with clear reference outputs.  ### ✅ **Human Evaluation** - Gold standard for assessing **fluency, relevance, coherence, and creativity**. - But expensive and not easily scalable.  ### ✅ **LLM-Powered Auto-Raters** - Use another calibrated language model to **simulate human evaluation**. - Can scale more easily than human raters. - Requires **calibration against human judgments** to ensure reliability.  ---  ## 🔹 Final Thoughts  So, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:  - **Similarity-based automatic metrics** (like BLEU/ROUGE), - **Human assessments**, and - **LLM-based scoring systems**.  This ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.  ---  Would you like a visual summary or code example showing how BLEU or ROUGE is computed in practice? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_oneline = query.replace(\"\\n\", \" \")\n",
    "## Concise answer prompt\n",
    "# prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
    "# Be sure to respond in a complete sentence, being concise, including only relevant information. \n",
    "# However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
    "# strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "# QUESTION: {query_oneline}\n",
    "# \"\"\"\n",
    "\n",
    "## Detailed answer prompt\n",
    "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. \n",
    "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \n",
    "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \n",
    "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "QUESTION: {query_oneline}\n",
    "\"\"\"\n",
    "\n",
    "# Add the retrieved documents to the prompt.\n",
    "for passage in all_passages:\n",
    "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
    "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate response from the query result using any LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate('llama3.2', prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "BLEU and ROUGE, two popular metrics used to evaluate the quality of text generated by Large Language Models (LLMs), have several limitations that make them less reliable for certain tasks. While they are useful for evaluating the precision and recall of a model's output, they can penalize valid but novel responses and struggle with semantic equivalence, which means they don't truly understand the meaning behind the words. This makes them less suitable for creative or open-ended tasks like story generation, Q&A, or chatbot interactions, where multiple answers can be equally valid.\n",
       "\n",
       "For example, if a model generates a response that is correct but not identical to the reference answer, BLEU and ROUGE will likely score it lower. Similarly, if the model captures key phrases from the original content in its summary, ROUGE will measure the overlap, but may not necessarily evaluate the summary's coherence or relevance.\n",
       "\n",
       "To get a comprehensive understanding of an LLM's performance, it's essential to combine automatic metrics like BLEU and ROUGE with human evaluations and LLM-based scoring systems. This ensures that both objective correctness and subjective quality are properly captured, which is crucial when working with LLMs in nuanced or high-stakes domains.\n",
       "\n",
       "In summary, while BLEU and ROUGE are valuable tools for evaluating LLM outputs, they should be used in conjunction with other evaluation methods to ensure a more complete picture of the model's performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic RAG (Manual, Not as acurate as Expected)\n",
    "\n",
    "### Key Idea -:\n",
    "1. Split the documents into sentences based on separators(.,?,!)\n",
    "2. Index each sentence based on position.\n",
    "3. Group: Choose how many sentences to be on either side. Add a buffer of sentences on either side of our selected sentence.\n",
    "4. Calculate distance between group of sentences.\n",
    "5. Merge groups based on similarity i.e. keep similar sentences together.\n",
    "6. Split the sentences that are not similar.\n",
    "\n",
    "Three different strategies could be used use on Semantic Chunking:\n",
    "\n",
    "- percentile — In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split.\n",
    "\n",
    "- standard_deviation — In this method, any difference greater than X standard deviations is split.\n",
    "\n",
    "- interquartile — In this method, the interquartile distance is used to split chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    # Split by punctuation followed by space or end of text\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_groups(indexed_sentences, buffer=2):\n",
    "    groups = []\n",
    "    for i in range(len(indexed_sentences)):\n",
    "        start = max(0, i - buffer)\n",
    "        end = min(len(indexed_sentences), i + buffer + 1)\n",
    "        group = \" \".join([s for _, s in indexed_sentences[start:end]])\n",
    "        groups.append((i, group))\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_groups(groups):\n",
    "    texts = [g[1] for g in groups]\n",
    "    # embeddings = embed_fn(texts)['embeddings']  # Output from Ollama or similar\n",
    "    text_embeddings = embed(model='bge-m3', input=texts)\n",
    "    embeddings = text_embeddings['embeddings']\n",
    "    return embeddings\n",
    "\n",
    "def compute_distances(embeddings):\n",
    "    return [cosine_distances([embeddings[i]], [embeddings[i+1]])[0][0] for i in range(len(embeddings)-1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy A: `percentile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_percentile(distances, percentile=95):\n",
    "    threshold = np.percentile(distances, percentile)\n",
    "    return [i for i, d in enumerate(distances) if d > threshold]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy B: `standard_deviation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_stddev(distances, num_std=2):\n",
    "    mean = np.mean(distances)\n",
    "    std = np.std(distances)\n",
    "    return [i for i, d in enumerate(distances) if d > mean + num_std * std]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy C: `interquartile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_interquartile(distances):\n",
    "    q1 = np.percentile(distances, 25)\n",
    "    q3 = np.percentile(distances, 75)\n",
    "    iqr = q3 - q1\n",
    "    threshold = q3 + 1.5 * iqr\n",
    "    return [i for i, d in enumerate(distances) if d > threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks_from_splits(sentences, split_indices):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for split in split_indices:\n",
    "        chunk_sentences = [s[1] if isinstance(s, tuple) else s for s in sentences[start:split+1]]\n",
    "        chunks.append(\" \".join(chunk_sentences))\n",
    "        start = split+1\n",
    "    if start < len(sentences):\n",
    "        chunk_sentences = [s[1] if isinstance(s, tuple) else s for s in sentences[start:]]\n",
    "        chunks.append(\" \".join(chunk_sentences))\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into sentences\n",
    "indexed_sentences = []\n",
    "for doc in documents:\n",
    "    indexed_sentences += [(i, sentence) for i, sentence in enumerate(split_into_sentences(str(doc)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Sure!'),\n",
       " (1,\n",
       "  'Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs.'),\n",
       " (2, \"They're commonly applied in these contexts:\\n\\n### 1.\"),\n",
       " (3,\n",
       "  '**Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2.'),\n",
       " (4,\n",
       "  '**Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a.'),\n",
       " (5,\n",
       "  '**golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3.'),\n",
       " (6,\n",
       "  '**Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs.'),\n",
       " (7,\n",
       "  'The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?'),\n",
       " (0,\n",
       "  'Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words.'),\n",
       " (1,\n",
       "  \"There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere's a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1.\"),\n",
       " (2,\n",
       "  'Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a.'),\n",
       " (3,\n",
       "  \"**Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn't capture context dynamically.\\n\\n#### b.\"),\n",
       " (4,\n",
       "  '**Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2.'),\n",
       " (5,\n",
       "  'Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a.'),\n",
       " (6,\n",
       "  '**Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b.'),\n",
       " (7,\n",
       "  '**GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c.'),\n",
       " (8,\n",
       "  '**FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3.'),\n",
       " (9,\n",
       "  'Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a.'),\n",
       " (10,\n",
       "  '**ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b.'),\n",
       " (11,\n",
       "  '**BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c.'),\n",
       " (12,\n",
       "  '**Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step).'),\n",
       " (0,\n",
       "  \"Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\\n\\nHere's a breakdown of its significance in the context of LLMs:\\n\\n*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs.\"),\n",
       " (1,\n",
       "  'Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\\n\\n*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**.'),\n",
       " (2,\n",
       "  'It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs.'),\n",
       " (3,\n",
       "  'For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\\n\\n*   **How Self-Attention Works:** The process involves several steps:\\n    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should \\'attend\\' to other words.'),\n",
       " (4,\n",
       "  'This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\\n    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights.'),\n",
       " (5,\n",
       "  'These weights indicate the strength of connection between words.\\n    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\\n    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\\n\\n*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel.'),\n",
       " (6,\n",
       "  'Each \\'head\\' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\\n\\n*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\\n\\n*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply.'),\n",
       " (7,\n",
       "  'Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\\n\\n*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process.'),\n",
       " (8,\n",
       "  'Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\\n\\nIn summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks.'),\n",
       " (9,\n",
       "  'Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = get_sentence_groups(list(indexed_sentences), buffer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They\\'re commonly applied in these contexts:\\n\\n### 1.'),\n",
       " (1,\n",
       "  'Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They\\'re commonly applied in these contexts:\\n\\n### 1. **Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2.'),\n",
       " (2,\n",
       "  'Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They\\'re commonly applied in these contexts:\\n\\n### 1. **Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a.'),\n",
       " (3,\n",
       "  'Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs. They\\'re commonly applied in these contexts:\\n\\n### 1. **Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3.'),\n",
       " (4,\n",
       "  'They\\'re commonly applied in these contexts:\\n\\n### 1. **Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3. **Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs.'),\n",
       " (5,\n",
       "  '**Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2. **Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3. **Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice?'),\n",
       " (6,\n",
       "  '**Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a. **golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3. **Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice? Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words.'),\n",
       " (7,\n",
       "  '**golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3. **Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice? Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere\\'s a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1.'),\n",
       " (8,\n",
       "  '**Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice? Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere\\'s a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1. Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a.'),\n",
       " (9,\n",
       "  \"The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice? Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere's a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1. Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a. **Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn't capture context dynamically.\\n\\n#### b.\"),\n",
       " (10,\n",
       "  \"Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words. There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere's a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1. Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a. **Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn't capture context dynamically.\\n\\n#### b. **Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2.\"),\n",
       " (11,\n",
       "  \"There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere's a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1. Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a. **Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn't capture context dynamically.\\n\\n#### b. **Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2. Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a.\"),\n",
       " (12,\n",
       "  \"Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a. **Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn't capture context dynamically.\\n\\n#### b. **Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2. Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a. **Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b.\"),\n",
       " (13,\n",
       "  \"**Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn't capture context dynamically.\\n\\n#### b. **Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2. Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a. **Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b. **GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c.\"),\n",
       " (14,\n",
       "  '**Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2. Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a. **Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b. **GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c. **FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3.'),\n",
       " (15,\n",
       "  'Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a. **Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b. **GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c. **FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3. Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a.'),\n",
       " (16,\n",
       "  '**Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b. **GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c. **FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3. Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a. **ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b.'),\n",
       " (17,\n",
       "  '**GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c. **FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3. Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a. **ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c.'),\n",
       " (18,\n",
       "  '**FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3. Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a. **ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c. **Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step).'),\n",
       " (19,\n",
       "  'Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a. **ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c. **Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step). Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\\n\\nHere\\'s a breakdown of its significance in the context of LLMs:\\n\\n*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs.'),\n",
       " (20,\n",
       "  '**ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b. **BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c. **Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step). Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\\n\\nHere\\'s a breakdown of its significance in the context of LLMs:\\n\\n*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs. Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\\n\\n*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**.'),\n",
       " (21,\n",
       "  '**BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c. **Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step). Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\\n\\nHere\\'s a breakdown of its significance in the context of LLMs:\\n\\n*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs. Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\\n\\n*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**. It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs.'),\n",
       " (22,\n",
       "  '**Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step). Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\\n\\nHere\\'s a breakdown of its significance in the context of LLMs:\\n\\n*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs. Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\\n\\n*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**. It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs. For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\\n\\n*   **How Self-Attention Works:** The process involves several steps:\\n    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should \\'attend\\' to other words.'),\n",
       " (23,\n",
       "  'Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\\n\\nHere\\'s a breakdown of its significance in the context of LLMs:\\n\\n*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs. Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\\n\\n*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**. It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs. For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\\n\\n*   **How Self-Attention Works:** The process involves several steps:\\n    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should \\'attend\\' to other words. This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\\n    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights.'),\n",
       " (24,\n",
       "  'Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\\n\\n*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**. It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs. For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\\n\\n*   **How Self-Attention Works:** The process involves several steps:\\n    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should \\'attend\\' to other words. This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\\n    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights. These weights indicate the strength of connection between words.\\n    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\\n    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\\n\\n*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel.'),\n",
       " (25,\n",
       "  'It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs. For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\\n\\n*   **How Self-Attention Works:** The process involves several steps:\\n    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should \\'attend\\' to other words. This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\\n    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights. These weights indicate the strength of connection between words.\\n    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\\n    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\\n\\n*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel. Each \\'head\\' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\\n\\n*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\\n\\n*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply.'),\n",
       " (26,\n",
       "  'For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\\n\\n*   **How Self-Attention Works:** The process involves several steps:\\n    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should \\'attend\\' to other words. This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\\n    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights. These weights indicate the strength of connection between words.\\n    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\\n    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\\n\\n*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel. Each \\'head\\' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\\n\\n*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\\n\\n*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply. Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\\n\\n*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process.'),\n",
       " (27,\n",
       "  'This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\\n    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights. These weights indicate the strength of connection between words.\\n    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\\n    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\\n\\n*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel. Each \\'head\\' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\\n\\n*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\\n\\n*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply. Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\\n\\n*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process. Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\\n\\nIn summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks.'),\n",
       " (28,\n",
       "  'These weights indicate the strength of connection between words.\\n    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\\n    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\\n\\n*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel. Each \\'head\\' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\\n\\n*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\\n\\n*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply. Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\\n\\n*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process. Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\\n\\nIn summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks. Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.'),\n",
       " (29,\n",
       "  'Each \\'head\\' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\\n\\n*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\\n\\n*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply. Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\\n\\n*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process. Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\\n\\nIn summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks. Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.'),\n",
       " (30,\n",
       "  'Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\\n\\n*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process. Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\\n\\nIn summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks. Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_groups(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = compute_distances(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.014861479771087205),\n",
       " np.float64(0.6242932450524188),\n",
       " np.float64(0.31781167707691527),\n",
       " np.float64(0.19440118284302832),\n",
       " np.float64(0.30915355963671864),\n",
       " np.float64(0.07747864256306847),\n",
       " np.float64(0.10225654671764428),\n",
       " np.float64(0.02980765548101716),\n",
       " np.float64(0.47633028816037215),\n",
       " np.float64(0.1470827182584915),\n",
       " np.float64(0.03461220532224285),\n",
       " np.float64(0.12691344442639108),\n",
       " np.float64(0.07195052112201783),\n",
       " np.float64(0.14416966991275137),\n",
       " np.float64(0.14256233054457668),\n",
       " np.float64(0.08367140675740092),\n",
       " np.float64(0.15517983843461058),\n",
       " np.float64(0.5864761692184371),\n",
       " np.float64(0.247394926088923),\n",
       " np.float64(0.06169298272705537),\n",
       " np.float64(0.2675377873066991),\n",
       " np.float64(0.41050125420073114),\n",
       " np.float64(0.5596207794418939),\n",
       " np.float64(0.0890455808314572),\n",
       " np.float64(0.587182870487359),\n",
       " np.float64(0.42110101697568403),\n",
       " np.float64(0.23969887485799624),\n",
       " np.float64(0.42974350368931713),\n",
       " np.float64(0.6225042978472564),\n",
       " np.float64(0.09863889173158724)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_by_percentile(distances, percentile=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 8, 9, 13, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = make_chunks_from_splits(list(indexed_sentences), splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs.',\n",
       " \"They're commonly applied in these contexts:\\n\\n### 1.\",\n",
       " '**Prompt Engineering**\\n- When experimenting with different prompts to elicit better responses from an LLM:\\n  - BLEU/ROUGE can **score how close the output is to a desired answer**.\\n  - This is useful for **automated prompt selection** (i.e., choosing the best prompt without needing human judgment every time).\\n\\n### 2.',\n",
       " '**Domain-Specific Tasks (e.g., Med-PaLM, SecLM)**\\n- In complex domains like medicine or security:\\n  - There may not be one \"correct\" answer.\\n  - BLEU/ROUGE can help by comparing generated answers to **a set of high-quality reference answers** (a.k.a.',\n",
       " '**golden responses**).\\n  - Helps to **quantify** performance even when human evaluation is expensive or time-consuming.\\n\\n### 3. **Text Generation (Machine Translation, Summarization)**\\n- In foundational NLP tasks:\\n  - **BLEU is used for translations** — does the output match human-translated sentences?\\n  - **ROUGE is used for summaries** — does the summary include key phrases from the original content?\\n\\n---\\n\\n## 🔹 Limitations of BLEU and ROUGE\\n\\nWhile useful, these metrics have **important limitations**, especially for **creative or open-ended tasks**:\\n\\n- They **penalize valid but novel responses** — because the generated text might be correct or high quality, even if it doesn\\'t match the reference exactly.\\n- They **struggle with semantic equivalence** — they don’t \"understand meaning\", just surface word overlaps.\\n- This makes them **less reliable for tasks like story generation, Q&A, or chatbot interactions**, where many answers can be equally valid.\\n\\n---\\n\\n## 🔹 The Bigger Picture: Evaluating LLMs\\n\\nTo properly evaluate LLMs, especially in complex or real-world applications, a **multi-method evaluation strategy** is recommended:\\n\\n### ✅ **Traditional Metrics (like BLEU/ROUGE)**\\n- Quick, objective, and reproducible.\\n- Best for **structured tasks** with clear reference outputs.\\n\\n### ✅ **Human Evaluation**\\n- Gold standard for assessing **fluency, relevance, coherence, and creativity**.\\n- But expensive and not easily scalable.\\n\\n### ✅ **LLM-Powered Auto-Raters**\\n- Use another calibrated language model to **simulate human evaluation**.\\n- Can scale more easily than human raters.\\n- Requires **calibration against human judgments** to ensure reliability.\\n\\n---\\n\\n## 🔹 Final Thoughts\\n\\nSo, while **BLEU and ROUGE are valuable tools**, especially in tasks like translation and summarization, they are **not enough by themselves** for a full evaluation of LLM outputs. The best evaluations combine:\\n\\n- **Similarity-based automatic metrics** (like BLEU/ROUGE),\\n- **Human assessments**, and\\n- **LLM-based scoring systems**.\\n\\nThis ensures that both **objective correctness** and **subjective quality** are properly captured — especially important when working with LLMs in nuanced or high-stakes domains.\\n\\n---\\n\\nWould you like a visual summary or code example showing how BLEU or ROUGE is computed in practice? Word embeddings are dense vector representations of words that capture their meanings, syntactic properties, and relationships with other words.',\n",
       " \"There are several types of word embeddings, generally categorized based on how they are learned and what kind of data they use.\\n\\nHere's a breakdown of the main types of word embeddings:\\n\\n---\\n\\n### **1.\",\n",
       " \"Count-Based Embeddings (Matrix Factorization)**\\nThese are derived from word co-occurrence matrices.\\n\\n#### a. **Latent Semantic Analysis (LSA)**\\n- **Method**: Create a term-document matrix, then apply **Singular Value Decomposition (SVD)** to reduce dimensions.\\n- **Captures**: Word similarity based on co-occurrence.\\n- **Limitations**: Linear, doesn't capture context dynamically.\\n\\n#### b. **Pointwise Mutual Information (PMI) + SVD**\\n- Uses **PMI matrix** (measuring association between words) and then reduces it using SVD.\\n- **Variants**: PPMI (Positive PMI), Shifted PMI, etc.\\n\\n---\\n\\n### **2. Predictive Embeddings (Neural Networks)**\\nThese use shallow neural networks to predict word context.\\n\\n#### a.\",\n",
       " '**Word2Vec**\\n- **Models**:\\n  - **CBOW (Continuous Bag-of-Words)**: Predicts a word from its context.\\n  - **Skip-Gram**: Predicts context words from a given word.\\n- **Training**: Negative sampling or hierarchical softmax.\\n- **Captures**: Semantic similarity, analogies (`king - man + woman ≈ queen`).\\n\\n```python\\n# Using gensim to load Word2Vec\\nfrom gensim.models import Word2Vec\\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\\n```\\n\\n#### b. **GloVe (Global Vectors for Word Representation)**\\n- **Method**: Factorizes a co-occurrence matrix with a cost function involving word probabilities.\\n- **Captures**: Global co-occurrence and local context.\\n- **Advantage**: Incorporates both count-based and predictive elements.\\n\\n#### c. **FastText**\\n- **Extension of Word2Vec** developed by Facebook.\\n- **Key Feature**: Represents words as bags of character **n-grams**, allowing it to handle **out-of-vocabulary (OOV)** words.\\n- Better for morphologically rich languages.\\n\\n---\\n\\n### **3.',\n",
       " 'Contextual Embeddings**\\nThese generate embeddings **based on context**, meaning the same word can have different vectors depending on its sentence.\\n\\n#### a.',\n",
       " '**ELMo (Embeddings from Language Models)**\\n- Uses a **bi-directional LSTM** over the entire sentence.\\n- **Context-aware**: Embeddings vary by usage.\\n\\n#### b.',\n",
       " '**BERT (Bidirectional Encoder Representations from Transformers)**\\n- Uses Transformer encoders.\\n- Learns embeddings for entire sentences, with **deep bidirectional** context.\\n- Typically uses the output from an intermediate or final layer for word/sentence embeddings.\\n\\n```python\\n# Example using Hugging Face Transformers\\nfrom transformers import BertTokenizer, BertModel\\nimport torch\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\\n\\ninputs = tokenizer(\"Example sentence\", return_tensors=\"pt\")\\noutputs = model(**inputs)\\nword_embeddings = outputs.last_hidden_state\\n```\\n\\n#### c. **Other Transformer Models**\\n- **GPT**, **RoBERTa**, **XLNet**, **T5**, etc.\\n- All provide contextual embeddings, fine-tuned for various tasks.\\n\\n---\\n\\n### Summary Table\\n\\n| Type              | Examples         | Contextual | Handles OOV | Architecture        |\\n|-------------------|------------------|------------|-------------|---------------------|\\n| Count-based       | LSA, PMI         | ❌         | ❌          | Matrix factorization |\\n| Predictive        | Word2Vec, GloVe  | ❌         | FastText: ✅| Shallow NN          |\\n| Contextual        | ELMo, BERT, GPT  | ✅         | ✅          | RNNs / Transformers |\\n\\n---\\n\\nLet me know if you want a visual comparison or a deeper dive into how one of them works (like training Word2Vec step-by-step).',\n",
       " \"Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\\n\\nHere's a breakdown of its significance in the context of LLMs:\\n\\n*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs.\",\n",
       " 'Unlike Recurrent Neural Networks (RNNs) that process sequences sequentially, **transformers can process sequences of tokens in parallel due to the self-attention mechanism**.\\n\\n*   **Understanding Context and Dependencies:** Self-attention enables LLMs to **focus on specific parts of the input sequence that are relevant to the task at hand**.',\n",
       " 'It allows the model to **capture long-range dependencies within sequences more effectively** than traditional RNNs. For example, in the sentence \"The tiger jumped out of a tree to get a drink because it was thirsty,\" self-attention helps the model understand that \"the tiger\" and \"it\" refer to the same entity by determining the relationships between different words.\\n\\n*   **How Self-Attention Works:** The process involves several steps:\\n    *   **Calculating Scores:** For each word in the input sequence, scores are calculated to determine how much it should \\'attend\\' to other words.',\n",
       " 'This is done by taking the **dot product of the query vector of one word with the key vectors of all the words in the sequence**.\\n    *   **Normalization:** The scores are then **divided by the square root of the key vector dimension** for stability and passed through a **softmax function** to obtain attention weights.',\n",
       " 'These weights indicate the strength of connection between words.\\n    *   **Weighted Values:** Each **value vector is multiplied by its corresponding attention weight**, and the results are summed up to produce a **context-aware representation for each word**.\\n    *   In practice, these calculations are performed simultaneously using matrices for queries (Q), keys (K), and values (V).\\n\\n*   **Multi-Head Attention:** Most LLMs utilize **multi-head attention**, which employs **multiple sets of query, key, and value weight matrices** running in parallel.',\n",
       " 'Each \\'head\\' can potentially focus on different aspects of the input relationships, and their outputs are combined to provide the model with a **richer representation of the input sequence**, improving its ability to handle complex language patterns.\\n\\n*   **Foundation for Large Reasoning Models:** The \"Foundational Large Language Models & Text Generation\" whitepaper notes that **transformer architectures, with their self-attention mechanisms, are foundational for achieving robust reasoning capabilities in large models**.\\n\\n*   **Evolution of Architectures:** Models like **BERT** (Bidirectional Encoder Representations from Transformers) are encoder-only architectures that heavily rely on the self-attention mechanism to understand context deeply.',\n",
       " 'Other architectures like the original Transformer and GPT models also utilize self-attention in their encoder and/or decoder components.\\n\\n*   **Efficiency Considerations:** While powerful, the self-attention mechanism in the original transformer has a **computational cost that is quadratic in the context length**, which can limit the size of the input it can effectively process.',\n",
       " 'Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\\n\\nIn summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks. Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"SemanticRAG01\"\n",
    "chroma_client = chromadb.Client()\n",
    "embed_fn = OllamaEmbeddingFunction()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
    "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['0'],\n",
       " 'embeddings': array([[-0.05443733,  0.00357789, -0.03386477, ..., -0.03299839,\n",
       "          0.06081931, -0.03797556]], shape=(1, 1024)),\n",
       " 'documents': ['Sure! Let’s break this down into a **clear, structured explanation** of what BLEU and ROUGE are, how they’re used, and where they fit in the **larger picture of evaluating Large Language Models (LLMs)** — especially in tasks like prompt engineering, machine translation, summarization, or domain-specific applications.\\n\\n---\\n\\n## 🔹 What Are BLEU and ROUGE?\\n\\n### **BLEU (Bilingual Evaluation Understudy)**\\n- Originally designed to **evaluate machine translation**.\\n- Measures **how similar a generated sentence is to one or more reference sentences**, based on matching sequences of words (n-grams).\\n- Focuses on **precision** — how much of the generated text overlaps with the reference.\\n\\n**Example:**\\nIf the model generates:  \\n`\"The cat is on the mat\"`  \\nand the reference is:  \\n`\"The cat sat on the mat\"`  \\nBLEU looks at the overlapping n-grams (like `the`, `cat`, `on`, `the`, `mat`), and calculates a score.\\n\\n### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\\n- Developed for **evaluating automatic summarization**.\\n- Measures **how much of the reference text appears in the generated text**, again using n-grams.\\n- Focuses more on **recall** — how much of the reference content is captured.\\n\\n---\\n\\n## 🔹 How Are These Metrics Used in Evaluating LLMs?\\n\\nBLEU and ROUGE are **automatic, quantitative metrics** used to evaluate the quality of text generated by LLMs.'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [None],\n",
       " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.peek(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"what are the limitations of BLEU and ROUGE\"\n",
    "# query = \"what are the different types of word embeddings in NLP\"\n",
    "query = \"what is self attention in LLMs\"\n",
    "result = db.query(query_texts=[query], n_results=2)\n",
    "[all_passages] = result[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage 0:\n",
      "Techniques like **Flash Attention** aim to optimize the self-attention calculation to improve latency and reduce costs.\n",
      "\n",
      "In summary, the **self-attention mechanism is a fundamental building block of modern LLMs**, enabling them to process information in parallel, understand contextual relationships between words in a sequence, capture long-range dependencies, and ultimately achieve strong performance in various natural language understanding and generation tasks. Its effectiveness has led to its widespread adoption in the Transformer architecture and its derivatives, which power many of the state-of-the-art LLMs discussed in the sources.\n",
      "\n",
      "Passage 1:\n",
      "Based on the sources, the **Self-Attention Mechanism** is a **crucial component** within the architecture of **Large Language Models (LLMs)**, particularly those based on the **Transformer architecture**.\n",
      "\n",
      "Here's a breakdown of its significance in the context of LLMs:\n",
      "\n",
      "*   **Core Mechanism in Transformers:** The self-attention mechanism is a **key innovation of the Transformer network**, which has become the foundation for most modern LLMs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_passages)):\n",
    "    print(f\"Passage {i}:\")\n",
    "    print(all_passages[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(db.count()):\n",
    "    db.delete(str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
